
@article{Farhart2023Analyses,
  abstract = {The introduction of the AI-powered chatbot ChatGPT by OpenAI has sparked much interest and debate among academic researchers. Commentators from different scientific disciplines have raised many concerns and issues, especially related to the ethics of using these tools in scientific writing and publications. In addition, there has been discussions about whether ChatGPT is trustworthy, effective, and useful in increasing researchers’ productivity. Therefore, in this paper, we evaluate ChatGPT’s performance on tasks related to bibliometric analysis, by comparing the output provided by the chatbot with a recently conducted bibliometric study on the same topic. The findings show that there are large discrepancies and ChatGPT’s trustworthiness is low in this particular area. Therefore, researchers should exercise caution when using ChatGPT as a tool in bibliometric studies.},
  author = {Farhart, Faiza and Saquib Sohail, Shahab and Øivind, Madsen},
  doi = {10.1080/23311916.2023.2222988},
  journal = {Cogent Engineering},
  keywords = {type:evaluation, chatgpt, bibliometrics, trustworthiness, artificial_intelligence, chatbots},
  number = {01},
  publisher = {Taylor & Francis Online},
  volume = {10},
  series = {CS-AIML},
  title = {How trustworthy is ChatGPT? The case of bibliometric analyses},
  year = {2023}
}

@article{Farina2023Review,
  abstract = {We review and critically assess several issues arising from the potential -large-scale- implementation or deployment of Large Language Models (LLMs) in society. These include security, political, economic, cultural, and educational issues as well as issues concerning social biases, creativity, copyright, and freedom of speech. We argue, without a preconceived pessimism toward these tools, that they may bring about many benefits. However, we also call for a balance assessment of their downsides. While our work is only preliminary and certainly partial it nevertheless holds some value as one of the first exploratory attempts in the literature.},
  author = {Farina, Mirko and Lavazza, Andrea},
  doi = {10.3389/frai.2023.1130913},
  journal = {Frontiers in Artificial Intelligence},
  keywords = {type:perspective, ai, chatgpt, openai, copyright, creativity, ethics, machine learning, social_biases},
  number = {02},
  publisher = {Frontiers},
  volume = {6},
  series = {Front. Artif. Intell.},
  title = {ChatGPT in society: emerging issues},
  year = {2023}
}

@article{Albayati2024Acceptance,
  abstract = {This study examines the factors influencing user acceptance of ChatGPT as a daily reference tool and assesses varying levels of user awareness. It aims to offer valuable insights into the potential benefits and challenges of implementing ChatGPT in an educational context. To achieve this objective, we employ an integrated model comprising the Technology Acceptance Model (TAM) and four novel external constructs: Privacy, Security, Social Influence, and Trust. This proposed model delivers an in-depth understanding of user acceptance by simultaneously measuring diverse user perspectives. Adopting a quantitative research approach, the study surveys undergraduate students regarding their use of ChatGPT. The results contribute to bridging the gap between technology and users, shedding light on users' actual experiences and considerations regarding AI-based tools. Specifically, the study is expected to reveal the significant influence of external factors on user acceptance of ChatGPT and to provide a set of recommendations for educational institutions, policymakers, and developers. This study aids the developers of ChatGPT and similar technologies by offering insights into how to design and enhance more user-friendly and secure systems that better meet users' needs and expectations.},
  author = {Albayati, Hayder},
  doi = {https://doi.org/10.1016/j.caeai.2024.100203},
  journal = {Computers & Education: Artificial Intelligence},
  keywords = {type:evaluation, chatgpt, tam, undergraduate_student_behaviour, privacy, security, social_influence, trust},
  number = {03},
  publisher = {Elsevier},
  volume = {6},
  series = {Comp. Educ. AI},
  title = {Investigating undergraduate students' perceptions and awareness of using ChatGPT as a regular assistance tool: A user acceptance perspective study},
  year = {2024}
}

@article{Choudhury2023Adoption,
  abstract = {Background ChatGPT (Chat Generative Pre-trained Transformer) has gained popularity for its ability to generate human-like responses. It is essential to note that overreliance or blind trust in ChatGPT, especially in high-stakes decision-making contexts, can have severe consequences. Similarly, lacking trust in the technology can lead to underuse, resulting in missed opportunities. Objective This study investigated the impact of users’ trust in ChatGPT on their intent and actual use of the technology. Four hypotheses were tested: (1) users’ intent to use ChatGPT increases with their trust in the technology; (2) the actual use of ChatGPT increases with users’ intent to use the technology; (3) the actual use of ChatGPT increases with users’ trust in the technology; and (4) users’ intent to use ChatGPT can partially mediate the effect of trust in the technology on its actual use. Methods This study distributed a web-based survey to adults in the United States who actively use ChatGPT (version 3.5) at least once a month between February 2023 through March 2023. The survey responses were used to develop 2 latent constructs: Trust and Intent to Use, with Actual Use being the outcome variable. The study used partial least squares structural equation modeling to evaluate and test the structural model and hypotheses. Results In the study, 607 respondents completed the survey. The primary uses of ChatGPT were for information gathering (n=219, 36.1%), entertainment (n=203, 33.4%), and problem-solving (n=135, 22.2%), with a smaller number using it for health-related queries (n=44, 7.2%) and other activities (n=6, 1%). Our model explained 50.5% and 9.8% of the variance in Intent to Use and Actual Use, respectively, with path coefficients of 0.711 and 0.221 for Trust on Intent to Use and Actual Use, respectively. The bootstrapped results failed to reject all 4 null hypotheses, with Trust having a significant direct effect on both Intent to Use (β=0.711, 95% CI 0.656-0.764) and Actual Use (β=0.302, 95% CI 0.229-0.374). The indirect effect of Trust on Actual Use, partially mediated by Intent to Use, was also significant (β=0.113, 95% CI 0.001-0.227). Conclusions Our results suggest that trust is critical to users’ adoption of ChatGPT. It remains crucial to highlight that ChatGPT was not initially designed for health care applications. Therefore, an overreliance on it for health-related advice could potentially lead to misinformation and subsequent health risks. Efforts must be focused on improving the ChatGPT’s ability to distinguish between queries that it can safely handle and those that should be redirected to human experts (health care professionals). Although risks are associated with excessive trust in artificial intelligence–driven chatbots such as ChatGPT, the potential risks can be reduced by advocating for shared accountability and fostering collaboration between developers, subject matter experts, and human factors researchers.},
  author = {Choudhury, Avishek and Shamszare, Hamid},
  doi = {10.2196/47184},
  journal = {Journal of Medical Internet Research},
  keywords = {type:evaluation, trust_in_ai, artificial_intelligence, technology_adoption, behavioral_intention, chatbot, human_factors, trust, adoption, intent, survey, shared_accountability, ai_policy},
  number = {04},
  publisher = {JMIR Publications},
  volume = {25},
  series = {JMIR},
  title = {Investigating the Impact of User Trust on the Adoption and Use of ChatGPT: A Survey Analysis},
  year = {2023}
}

@article{Ashfaq2020Intention,
  abstract = {Chatbots are mainly text-based conversational agents that simulate conversations with users. This study aims to investigate drivers of users` satisfaction and continuance intention toward chatbot-based customer service. We propose an analytical framework combining the expectation-confirmation model (ECM), information system success (ISS) model, TAM, and the need for interaction with a service employee (NFI-SE). Analysis of data collected from 370 actual chatbot users reveals that information quality (IQ) and service quality (SQ) positively influence consumers` satisfaction, and that perceived enjoyment (PE), perceived usefulness (PU), and perceived ease of use (PEOU) are significant predictors of continuance intention (CI). The need for interaction with an employee moderates the effects of PEOU and PU on satisfaction. The findings also revealed that satisfaction with chatbot e-service is a strong determinant and predictor of users` CI toward chatbots. Thus, chatbots should enhance their information and service quality to increase users` satisfaction. The findings imply that digital technologies services, such as chatbots, could be combined with human service employees to satisfy digital users.},
  author = {Ashfaq, Muhammad and Yu, Shubin and Maria Correia Loureiro, Sandra},
  doi = {10.1016/j.tele.2020.101473},
  journal = {Telematics and Informatics},
  keywords = {type:evaluation, chabots, continuance_intention, expectation_confirmation_model, information_systems_success, perceived_enjoyment, information_quality, perceived_usefulness, perceived_ease_of_use, determinant, test_access_mechanism},
  number = {05},
  publisher = {Elsevier},
  volume = {54},
  series = {T&I},
  title = {I, Chatbot: Modeling the Determinants of Users’ Satisfaction and Continuance Intention of AI-Powered Service Agents},
  year = {2020}
}

@article{Baek2023Creepiness,
  abstract = {Few studies have examined user motivations to use generative artificial intelligence (AI). This research aims to address this gap by examining how user motivations for ChatGPT usage affect perceived creepiness, trust, and the intention to continue using AI chatbot technology. The findings of an online survey (N = 421) reveal a negative relationship between personalization and creepiness, while task efficiency and social interaction are positively associated with creepiness. Increased levels of creepiness, in turn, result in decreased continuance intention. Furthermore, task efficiency and personalization have a positive impact on trust, leading to increased continuance intention. The results contribute to the field of human–computer interaction by investigating the motivations for utilizing generative AI chatbots and advancing our comprehension of AI creepiness, trust, and continuance intention. The practical ramifications of this research can inform the design of user interfaces and the development of features for generative AI chatbots.},
  author = {Hyun Baek, Tae and Kim, Minseong},
  doi = {10.1016/j.tele.2023.102030},
  journal = {Telematics and Informatics},
  keywords = {type:evaluation, chatgpt, generative_artificial_intelligence, uses_and_gratification_theory, creepiness, trust, continuance_intention},
  number = {06},
  publisher = {Elsevier},
  volume = {83},
  series = {T&I},
  title = {Is ChatGPT scary good? How user motivations affect creepiness and trust in generative artificial intelligence},
  year = {2023}
}

@article{Sohail2023Taxonomy,
  abstract = {Chat Generative Pre-trained Transformer (ChatGPT) has gained significant interest and attention since its launch in November 2022. It has shown impressive performance in various domains, including passing exams and creative writing. However, challenges and concerns related to biases and trust persist. In this work, we present a comprehensive review of over 100 Scopus-indexed publications on ChatGPT, aiming to provide a taxonomy of ChatGPT research and explore its applications. We critically analyze the existing literature, identifying common approaches employed in the studies. Additionally, we investigate diverse application areas where ChatGPT has found utility, such as healthcare, marketing and financial services, software engineering, academic and scientific writing, research and education, environmental science, and natural language processing. Through examining these applications, we gain valuable insights into the potential of ChatGPT in addressing real-world challenges. We also discuss crucial issues related to ChatGPT, including biases and trustworthiness, emphasizing the need for further research and development in these areas. Furthermore, we identify potential future directions for ChatGPT research, proposing solutions to current challenges and speculating on expected advancements. By fully leveraging the capabilities of ChatGPT, we can unlock its potential across various domains, leading to advancements in conversational AI and transformative impacts in society.},
  author = {Saquib Sohail, Shahab and Farhart, Faiza and Himeur, Yassine and Nadeem, Mohammad and Øivind, Madsen and Singh, Yashbir and Atalla, Shadi and Mansoor, Wathiq},
  doi = {10.1016/j.jksuci.2023.101675},
  journal = {Journal of King Saud University - Computer and Information Sciences},
  keywords = {type:review, chatgpt, large_language_models, generative_pre_trained_transformer, ai_generated_content, systematic_review, trustworthy_ai},
  number = {07},
  publisher = {Elsevier},
  volume = {35},
  series = {CIS},
  title = {Decoding ChatGPT: A taxonomy of existing research, current challenges, and possible future directions},
  year = {2023}
}

@article{Mohammed2023Decoding,
  abstract = {Artificial Intelligence (AI) technology has revolutionized how we interact with information and entertainment, with ChatGPT, a language model developed by OpenAI, being among its prominent applications. However, knowledge regarding the psychological impact of interacting with ChatGPT is limited. This study investigated the relationships between trust in ChatGPT; ChatGPT’s user perceptions; perceived stereotyping by ChatGPT; and two psychological outcomes, namely, psychological well-being and self-esteem. This study hypothesized that the former three variables exhibit a positive direct relationship with self-esteem. Additionally, the study proposed that job anxiety moderates the associations among trust in ChatGPT, user perceptions of ChatGPT, and psychological well-being. Using a survey design, data were collected from 732 participants and analyzed using SEM and SmartPLS analysis. Notably, perceived stereotyping by ChatGPT significantly predicted self-esteem, while user perceptions of ChatGPT and trust in ChatGPT exhibited a positive direct relationship with self-esteem. Additionally, job anxiety moderated the relationship between ChatGPT’s user perceptions and psychological well-being. These results provide important insights into the psychological effects of interacting with AI technology and highlight job anxiety’s role in moderating these effects. This study’s findings have implications for developing and using AI technology in various fields, including mental health and human-robot interactions.},
  author = {Mohammed, Salah and Hussam, Alhalbusi and Maria Mohd, Ismail and Fadi Abdelfattah},
  doi = {10.1007/s12144-023-04989-0},
  journal = {Current Psychology},
  keywords = {type:evaluation, chatgpt, job_anxiety, perceived_stereotyping, psychological_well_being, self_esteem, user_perception},
  number = {08},
  publisher = {Springer Nature},
  volume = {43},
  series = {PSE},
  title = {Chatting with ChatGPT: decoding the mind of Chatbot users and unveiling the intricate connections between user perception, trust and stereotype perception on self-esteem and psychological well-being},
  year = {2023}
}

@article{Ltifi2023Trust,
  abstract = {Today, the use of chatbots for different functions in various industries has become a very interesting business for companies. Chatbots are promising types of interfaces. It is therefore necessary to understand how customers interact with retailers' interfaces in order to provide them with a better experience. In this study, we mobilise two theories, such as Stimilus-Organism-Response and Social Presence Theory, to formulate our research hypotheses. We have made major contributions to the interactive marketing and artificial intelligence literature by focusing on an emerging interactive technology: text chatbots. Our aim is to test the hedonic attributes of consumer trust in text chatbots by integrating the social and emotional aspects of this interaction. We also want to look at the moderating effects of text chatbot disclosure and task complexity. Based on the responses, we ran a questionnaire survey. A total of 353 people were polled for data. Participants were chosen at random. The structural equation modelling technique was used. First, the findings revealed that empathy and friendliness are major hedonic predictors of consumers' confidence in text chatbots. Second, the results demonstrate that the chatbot's task complexity and disclosure partially affect the empathy-trust relationship and the usability-trust relationship. We have made significant contributions to the field of interactive marketing research and artificial intelligence by focusing on new interactive technologies such as text-based chatbots. Our study is one of the first to look at the hedonic determinants of customer belief in text-based chatbots (1). All previous research has concentrated on the practical application of chatbots for digital customer service. The moderating effects of human-chatbot contact are investigated in our study (2). These two contributions make our research original. The findings give additional information that e-service providers and chatbot developers may utilise to improve their services, understand their effects on user experience, and provide a guide for strategy development and relationship building.},
  author = {Ltifi, Moez},
  doi = {10.1186/s43093-023-00288-z},
  journal = {Future Business Journal},
  keywords = {type:evaluation, chatbot_empathy_and_usability, e-trust, moderating_effects, experience, structural_equation_method},
  number = {09},
  publisher = {Springer Open},
  volume = {9},
  series = {FBJ},
  title = {Trust in the chatbot: a semi-human relationship},
  year = {2023}
}


@article{Brennen2020Explainable,
  abstract = {This paper summarizes findings from a qualitative research effort aimed at understanding how various stakeholders characterize the problem of Explainable Artificial Intelligence (Explainable AI or XAI). During a nine-month period, the author conducted 40 interviews and 2 focus groups. An analysis of data gathered led to two significant initial findings: (1) current discourse on Explainable AI is hindered by a lack of consistent terminology; and (2) there are multiple distinct use cases for Explainable AI, including: debugging models, understanding bias, and building trust. These uses cases assume different user personas, will likely require different explanation strategies, and are not evenly addressed by current XAI tools. This stakeholder research supports a broad characterization of the problem of Explainable AI and can provide important context to inform the design of future capabilities.},
  author = {Brennen, Andrea},
  doi = {10.1145/3334480.3383047},
  journal = {CHI EA '20},
  keywords = {type:evaluation, explainable_ai, user_research, machine_learning, ui/ux_design, data_science, interface_design},
  number = {10},
  publisher = {Association for Computing Machinery},
  series = {CHI Conference on Human Factors in Computing Systems},
  title = {What Do People Really Want When They Say They Want "Explainable AI?" We Asked 60 Stakeholders.},
  year = {2020}
}